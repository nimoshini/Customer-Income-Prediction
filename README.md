ğŸš€ Income Prediction Assistant: Census Income Analysis

ğŸ“Œ Overview
The Income Prediction Assistant is an AI-powered application that predicts whether an individual earns more than $50K per year based on demographic and employment data. This binary classification problem is solved using machine learning models, with Random Forest emerging as the best-performing model.

ğŸ¯ Features
âœ… Predict Income Bracket: Uses AI models to determine if a person earns <=50K or >50K.
âœ… Multiple ML Models: Implements Logistic Regression, Decision Tree, and Random Forest.
âœ… Hyperparameter Optimization: Uses GridSearchCV for optimal Random Forest tuning.
âœ… Feature Engineering: Label-encodes categorical features for effective model training.
âœ… Performance Evaluation: Compares models based on accuracy and generalization ability.

ğŸ“Š Dataset Structure
ğŸ“Œ Target Variable
<=50K (Encoded as 0)

>50K (Encoded as 1)

ğŸ”¢ Numerical Features
age: Individualâ€™s age

fnlwgt: Final weight assigned

education-num: Years of education

capital-gain: Capital gain in income

capital-loss: Capital loss in income

hours-per-week: Weekly working hours

ğŸ”  Categorical Features
workclass: Type of employer

education: Education level

marital-status: Marital status

occupation: Job type

relationship: Family role

race: Ethnicity

sex: Gender

native-country: Country of origin

ğŸ“Œ All categorical features were label-encoded before training.

ğŸ› ï¸ Models & Performance
Model	Accuracy
ğŸ“‰ Logistic Regression	82.79%
ğŸŒ³ Decision Tree	86.00%
ğŸŒ² Random Forest	86.44%
ğŸ“‰ Logistic Regression
ğŸ“Œ Description:

Linear classification model using the sigmoid function to estimate probabilities.

Highly interpretable, showing how features impact predictions.

âš ï¸ Limitations:

Assumes linear relationships, which might not always hold.

Struggles with complex data patterns.

âœ” Accuracy: 82.79%

ğŸŒ³ Decision Tree Classifier
ğŸ“Œ Description:

Splits data into branches based on feature conditions.

Captures non-linear relationships and is easy to interpret.

ğŸ“Œ Optimization:

Tuned max_depth from 1 to 20 to prevent overfitting.

âš ï¸ Limitations:

Overfits when depth is too large.

Sensitive to small data variations.

âœ” Best max_depth: Found via brute-force search.
âœ” Accuracy: 86.00%

ğŸŒ² Random Forest Classifier (Best Model!)
ğŸ“Œ Description:

An ensemble learning method that builds multiple Decision Trees and averages outputs for better accuracy.

Reduces overfitting and generalizes well.

ğŸ“Œ Hyperparameter Optimization (GridSearchCV):

n_estimators (Number of trees): 50, 100, 200

max_depth: 10, 20, 30

min_samples_split: 2, 5, 10

ğŸ“Œ Best Parameters Found:
âœ” n_estimators: 200
âœ” max_depth: 20
âœ” min_samples_split: 5

ğŸš€ Advantages:
âœ… More robust than a single Decision Tree.
âœ… Handles imbalanced data well.
âœ… Captures complex data patterns effectively.

âš ï¸ Limitations:

Computationally expensive for large datasets.

Less interpretable than Decision Trees.

âœ” Accuracy: 86.44% (Best Model!)

ğŸ† Conclusion: Best Model Selection
ğŸ”¹ ğŸ† Random Forest is the best model with 86.44% accuracy.
ğŸ”¹ Effectively captures complex relationships while preventing overfitting.
ğŸ”¹ Optimized hyperparameters (via GridSearchCV) improve performance.

ğŸš€ Final Recommendation: Use Random Forest for the best accuracy!
